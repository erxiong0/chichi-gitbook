---
title: Proximal Policy Optimization (PPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Proximal Policy Optimization Algorithms

The **Proximal Policy Optimization (PPO)** algorithm, proposed by OpenAI in 2017 [[1]](#references), establishes a robust policy gradient framework that combines three core components: clipped objective functions, value estimation, and entropy regularization.

---

### Objective Function
The unified optimization objective integrates three key elements:
$$
L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]
$$

#### 1. Clipped Surrogate Objective
$$
L_t^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t,\ \text{clip}\left( r_t(\theta),\ 1-\epsilon,\ 1+\epsilon \right) \hat{A}_t \right) \right]
$$  
  
**Key Parameters**:
- Probability Ratio: $r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$  
  
- Advantage Estimate: $\hat{A}_t$ (computed via GAE)  
  
- Clipping Threshold: $\epsilon \in [0.1, 0.3]$

#### 2. Value Function Loss
$$
L_t^{\text{VF}}(\theta) = \frac{1}{2} \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right)^2 \right]
$$  
  
*Implementation Note*: The $\frac{1}{2}$ coefficient serves as a gradient scaling factor.  
  

#### 3. Entropy Regularization
$$
S[\pi_\theta](s_t) = -\sum_{a'} \pi_\theta(a'|s_t) \log \pi_\theta(a'|s_t)
$$  
  
**Coefficient Ranges**:  
    
- Value Loss Weight: $c_1 \in [0.5, 1.0]$
- Entropy Bonus Weight: $c_2 \in [0.01, 0.05]$

---

### Architecture & Execution Pipeline  
  
#### Network Architecture Components
- **Shared Backbone**  
  *Function*: Feature extraction    
  *Output*: Embedding vector $\mathbb{R}^{d_{\text{embed}}}$  
  
- **Policy Head**    
  *Function*: Generates action probability distribution    
  *Output*: $\pi_\theta(a|s) \in \Delta(\mathcal{A})$ (probability simplex over action space)  
  
- **Value Head**    
  *Function*: Predicts state-value    
  *Output*: Scalar $V_\theta(s) \in \mathbb{R}$  
  
---

### Execution Workflow

#### Phase 1: Experience Collection
1. **Trajectory Generation**  
   Agents interact with environment using $\pi_{\text{old}}$ to collect trajectories:  
   $$\tau = \{s_t, a_t, r_t\}_{t=0}^{T-1}$$

2. **Advantage Calculation**  
   Compute Generalized Advantage Estimation (GAE) [[1]](#references):  
   $$
   \begin{aligned}
   \delta_t &= r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t) \\
   \hat{A}_t &= \sum_{l=0}^{T-t-1} (\gamma\lambda)^l \delta_{t+l}
   \end{aligned}
   $$

3. **Buffer Storage**  
   Store processed transitions in replay buffer:  
   $$(s_t, a_t, \hat{A}_t, V_t^{\text{targ}})$$  
   *Capacity*: $10^5 \sim 10^6$ transitions

#### Phase 2: Parameter Update
For $K$ optimization epochs:
1. **Mini-batch Sampling**  
   Uniformly sample $B$ transitions ($B \in [64, 2048]$) from buffer.

2. **Policy Optimization**  
   Compute policy gradient with clipped ratios:  
   $$\nabla_\theta L^{\text{CLIP}}$$

3. **Value Network Update**  
   Minimize MSE loss via SGD:  
   $$
   \nabla_\theta L^{\text{VF}} = \mathbb{E}_{\text{buffer}} \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right) \nabla_\theta V_\theta(s_t) \right]
   $$

4. **Entropy Adaptation** (Optional)  
   Dynamically adjust $c_2$ based on entropy monitoring.

---

## References
1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.  
   DOI: [10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)