---
title: Proximal Policy Optimization (PPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Proximal Policy Optimization Algorithms

The **Proximal Policy Optimization (PPO)** algorithm, proposed by OpenAI in 2017 [[1]](#references), establishes a robust policy gradient framework that combines three core components: clipped objective functions, value estimation, and entropy regularization.

---

### Objective Function
The unified optimization objective integrates three key elements:  
    
$$L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]$$

#### 1. Clipped Surrogate Objective  
$$L_t^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t,\ \text{clip}\left( r_t(\theta),\ 1-\epsilon,\ 1+\epsilon \right) \hat{A}_t \right) \right]$$  
  
where $r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}$ is ratio paremeter, $\hat{A}_t$ is advantage estimate and $\epsilon \in [0.1, 0.3]$ a clipping threshold which contrains policy update in a safe limit.
  

#### 2. Value Function Loss  
$$L_t^{\text{VF}}(\theta) = \frac{1}{2} \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right)^2 \right]$$  
  
*Implementation Note*: The $\frac{1}{2}$ coefficient serves as a gradient scaling factor.  
  

#### 3. Entropy Regularization  
$$S[\pi_\theta](s_t) = -\sum_{a'} \pi_\theta(a'|s_t) \log \pi_\theta(a'|s_t)$$  
  
**Tips**:  
  Coefficients $c_1$ and $c_2$ balance the policy update, value function accuracy, and entropy bonus. Usually value loss weight is set to $c_1 \in [0.5, 1.0]$ and entropy bonus weight is $c_2 \in [0.01, 0.05]$.

---

### Architecture & Execution Pipeline  
  
#### Network Architecture Components
The policy network $\pi_\theta(a|s_t)$ and value function estimator $V_\theta(s_t)$ share a unified neural architecture with sepecialized components such as feature encoder
$h_t=f_\theta^{encoder}(s_t)$ with separate heads, e.g. policy head with $\pi_\theta(a|s_t)={\rm softmax}(W_\pi h_t+b_\pi)$ and output probability distribution over action $\mathcal{A}$ while value head maybe $V_\theta(s_t)=W_vh_t+b_v$ with a scalar value estimate.
  
---

### Execution Workflow

#### Phase 1: Experience Collection Pipeline  
During the rollout phase, agents interact with the environment using the old policy $\pi_{\text old}$ to generate trajectories $\tau=\{(s_t,a_t,r_t)\}^{T-1}_{t=0}$.  Subsequently, the Generalized Advantage Estimation (GAE) method computes advantage value $\hat{A}_t$ by through recursive temporal difference calculations:  
      
$$
\begin{aligned}
\delta_t &= r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t), \\
\hat{A}_t &= \sum_{\ell=0}^{T-t-1} (\gamma\lambda)^\ell \delta_{t+\ell},
\end{aligned}
$$  
  
where $\gamma$ is the discount factor and $\lambda$ the GAE smoothing coefficient. Processed transitions $(s_t,a_t,\hat{A}_t,V_t^{\rm targ})$ - with target values $V_t^{\rm targ}=\hat{A}_t+V_\theta(s_t)$ - are stored in an experience replay buffer (capacity: $10^5\sim 10^6$). This staged pipeline decouples data collection from policy updates while maintaining sample diversity through large-scale experience reuse. 

#### Phase 2: Parameter Update
For $K$ optimization epochs:
1. **Mini-batch Sampling**: Uniformly sample $B$ transitions ($B \in [64, 2048]$) from buffer.

2. **Policy Optimization**: Compute gradients for the clipped surrogate objective:
$$\nabla_\theta L^{CLIP}=\mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, {\text clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\nabla_\theta\log\pi_\theta(a_t|s_t)\right]$$

3. **Value Network Update**: Minimize the value function's MSE loss through SGD:
$$\nabla_\theta L^{VF}=\mathbb{E}_{(s_t, V_t^{\text targ})\sim {\text buffer}}\left[(V_\theta(s_t)-V_t^{\text targ}\nabla_\theta V_\theta(s_t))\right]$$

4. **Entropy Adaptation** (Optional): Dynamically adjust $c_2$ based on entropy monitoring.

---

## References
1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.  
   DOI: [10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)