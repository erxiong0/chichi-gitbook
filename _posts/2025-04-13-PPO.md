---
title: Proximal Policy Optimization (PPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Proximal Policy Optimization Algorithms

The **Proximal Policy Optimization (PPO)** algorithm, proposed by OpenAI in 2017 [[1]](#references), establishes a robust policy gradient framework that combines three core components: clipped objective functions, value estimation, and entropy regularization.

### Objective Function
The unified optimization objective integrates three key elements:
$$
L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]
$$

#### 1. Clipped Surrogate Objective
$$
L_t^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t,\ \text{clip}\left( r_t(\theta),\ 1-\epsilon,\ 1+\epsilon \right) \hat{A}_t \right) \right]
$$
**Parameters**:
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$ (Probability ratio)
- $\hat{A}_t$ (Generalized Advantage Estimate)
- $\epsilon \in [0.1, 0.3]$ (Clipping threshold)

#### 2. Value Function Loss
$$
L_t^{\text{VF}}(\theta) = \frac{1}{2} \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right)^2 \right]
$$
*Implementation Note*: The $\frac{1}{2}$ coefficient serves as gradient scaling factor absent in the original formulation.

#### 3. Entropy Regularization
$$
S[\pi_\theta](s_t) = -\sum_{a'} \pi_\theta(a'|s_t) \log \pi_\theta(a'|s_t)
$$
**Coefficient Ranges**:
- $c_1 \in [0.5, 1.0]$ (Value loss weight)
- $c_2 \in [0.01, 0.05]$ (Entropy bonus weight)

---

### Architecture & Execution Pipeline

#### Network Architecture
| Component          | Description                                  | Output Specification          |
|--------------------|----------------------------------------------|--------------------------------|
| Shared Backbone    | Feature extraction layers                    | $\mathbb{R}^{d_{\text{embed}}}$|
| Policy Head        | Action probability distribution             | $\pi_\theta(a\|s) \in \Delta(\mathcal{A})$ |
| Value Head         | State-value prediction                       | $V_\theta(s) \in \mathbb{R}$  |

#### Phase 1: Experience Collection
1. **Trajectory Generation**:  
   Agents interact with environment using $\pi_{\text{old}}$ to collect $\tau = \{s_t, a_t, r_t\}_{t=0}^{T-1}$
2. **Advantage Calculation**:  
   Compute Generalized Advantage Estimation (GAE) [[1]](#references):
   $$
   \begin{aligned}
   \delta_t &= r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t) \\
   \hat{A}_t &= \sum_{l=0}^{T-t-1} (\gamma\lambda)^l \delta_{t+l}
   \end{aligned}
   $$
3. **Buffer Storage**:  
   Store processed transitions $(s_t, a_t, \hat{A}_t, V_t^{\text{targ}})$ in replay buffer  
   (Capacity: $10^5 \sim 10^6$ transitions)

#### Phase 2: Parameter Update
For $K$ optimization epochs ($K \in [3, 10]$):
1. **Mini-batch Sampling**:  
   Uniformly sample $B$ transitions from buffer ($B \in [64, 2048]$)
2. **Policy Optimization**:  
   Calculate gradient $\nabla_\theta L^{\text{CLIP}}$ with clipped probability ratios
3. **Value Network Update**:  
   Minimize MSE loss through SGD:
   $$
   \nabla_\theta L^{\text{VF}} = \mathbb{E}_{\text{buffer}} \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right) \nabla_\theta V_\theta(s_t) \right]
   $$
4. **Entropy Adaptation** (Optional):  
   Adjust $c_2$ dynamically based on policy entropy level

---

## References
1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.  
   DOI: [10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)