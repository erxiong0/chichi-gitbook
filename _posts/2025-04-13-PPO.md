---
title: Proximal Policy Optimization (PPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Proximal Policy Optimization Algorithms

The **Proximal Policy Optimization (PPO)** algorithm, proposed by OpenAI in 2017 [[1]](#references), introduces a stabilized policy gradient method that improves upon the Trust Region Policy Optimization (TRPO) framework by combining clipped objective functions and entropy regularization.

### Objective Function
The composite objective function is defined as:
$$
L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]
$$

#### 1. Clipped Surrogate Objective ($L_t^{\text{CLIP}}$)
$$
L_t^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \ \text{clip}\left( r_t(\theta), 1-\epsilon, 1+\epsilon \right) \hat{A}_t \right) \right]
$$
where:
- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$ is the probability ratio
- $\hat{A}_t$ denotes the advantage estimate
- $\epsilon$ controls the clipping range (typically 0.1-0.3)

#### 2. Value Function Loss ($L_t^{\text{VF}}$)
$$
L_t^{\text{VF}}(\theta) = \frac{1}{2} \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right)^2 \right]
$$
*Implementation Note*: The original paper omits the $\frac{1}{2}$ coefficient, but practical implementations often include it for gradient scaling.

#### 3. Entropy Bonus ($S[\pi_\theta]$)
$$
S[\pi_\theta](s_t) = -\sum_{a'} \pi_\theta(a'|s_t) \log \pi_\theta(a'|s_t)
$$
Coefficients $c_1$ (typically 0.5-1.0) and $c_2$ (typically 0.01-0.05) balance policy optimization, value estimation, and exploration.

---

### Algorithmic Components

#### Policy-Value Network Architecture

--------------------------------------------------------------
| Component          | Description                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| **Shared Backbone** | Convolutional/Transformer layers for feature extraction                    |
| **Policy Head**    | Outputs action probabilities $\pi_\theta(a\|s)$ via softmax activation       |
| **Value Head**     | Predicts state value $V_\theta(s)$ through linear projection               |
--------------------------------------------------------------

#### Phased Execution
**1. Data Collection (Rollout Phase)**  
Agents interact with the environment using the current policy $\pi_{\text{old}}$ to generate trajectories.  
- Compute target values via Generalized Advantage Estimation (GAE) [[1]](#references):
  $$
  \begin{aligned}
  V_t^{\text{targ}} &= \hat{A}_t + V_\theta(s_t) \\
  \hat{A}_t &= \sum_{l=0}^{T-t} (\gamma\lambda)^l \delta_{t+l} \\
  \delta_t &= r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t)
  \end{aligned}
  $$
- Store transitions $(s_t, a_t, r_t, V_t^{\text{targ}})$ in a replay buffer (capacity: $10^5$-$10^6$ samples)

**2. Training Phase**  
For $K$ epochs:
1. Sample mini-batches from the replay buffer
2. Compute policy gradient with clipped objectives
3. Update value network via:
   $$
   \nabla_\theta L^{\text{VF}} = \mathbb{E}_{(s_t, V_t^{\text{targ}}) \sim \text{buffer}} \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right) \nabla_\theta V_\theta(s_t) \right]
   $$
4. Adjust entropy coefficient $c_2$ via adaptive methods if implemented

---

## References
1. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.  
   [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)