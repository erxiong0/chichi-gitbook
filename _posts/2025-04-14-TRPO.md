---
title: Trust Region Policy Optimization (TRPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Introduction

The **Trust Region Policy Optimization (TRPO)** algorithm[[1]](#references), introduced in 2017, addresses critical limitations of policy gradient methods in reinforcement learning by incorporating a **trust region constraint** to enable stable learning to enable stable learning of complex policies. During this period, policy optimization approaches predominantly fell into three categories:  
- *policy iteration methods* leverages dynamic programming  
- *policy gradient methods* utilizes explicit gradient estimates  
- *derivative-free optimization methods*  
  
While gradient-based techniques had demonstrated success in training function approximators for supervised learning, they exhibited inconsistent performance in policy optimization scenarios, frequently underperforming gradient-free approaches like random search. This discrepancy stemmed from challenges including **high variance in gradient estimates** and **non-monotonic policy improvement** during iterative updates. TRPO resolve these issues by mathematically formalizing policy updates as a constrained optimization problem, where the objective function maximization is bounded by a **Kullback-Leibler (KL) divergence threshold**. This mechanism enforces localized updates within a trust region, balancing aggressive policy improvement with stability guarantees to prevent catastrophic performance collapse.

---
### Background  
Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple ($\mathcal{S}, \mathcal{A}, P, r, \rho_0, \gamma$), where  
- $\mathcal{S}$ is a finite set of states  
- $\mathcal{A}$ is a finite set of actions  
- $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$ is the transition probability distribution  
- $r: \mathcal{S}\rightarrow\mathbb{R}$ is the reward function  
- $\rho_0:\mathcal{S}\rightarrow\mathbb{R}$ is the distribution of the initial state $s_0$  
- $\gamma\in(0,1)$ is the discount factor.  
  
Let $\pi$ denote a stochastic policy $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0,1]$, and let $\eta(\pi)$ denote its expected discounted reward:  
  
$$\eta(\pi)=\mathbb{E}_{s_0,a_0,...}\left[\sum_{t=0}^\infty\gamma^tr(s_t)\right]$$

where    
$s_0\sim \rho_0(s_0),\ a_t\sim \pi(a_t|s_t),\ s_{t+1}\sim P(s_{t+1}|s_t, a_t)$.  
  
The state-action value function $Q_\pi$, the value function $V_\pi$, and the advantage function $A_\pi$ are defined as followed:  
    
$$
\begin{aligned}
Q_\pi(s_t, a_t) &= \mathbb{E}_{s_{t+1}, a_{t+1},...}\left[\sum_{k=0}^\infty\gamma^kr(s_{t+k})\right]\\
V_\pi(s_t)&=\mathbb{E}_{a_t, s_{t+1},...}\left[\sum_{k=0}^\infty\gamma^kr(s_{t+k})\right]\\
A_\pi(s,a)&=Q_\pi(s,a)-V_\pi(s)
\end{aligned}
$$

where,  
$a_t\sim \pi(a_t|s_t),\ s_{t+1}\sim P(s_{t+1}|s_t, a_t)$ for $t\geq 0$.  
  
There's useful identity expresses the expected return of another policy $\tilde{\pi}$ in terms of the advantage over $\pi$, accumulated over timesteps (see Kakade & Langford (2022)[[2]](#references))  
   
$$\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0,a_0,...\sim\tilde{\pi}}\left[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)\right]\tag{1}$$

  
where the notation $\mathbb{E}_{s_0, a_0, ...,\sim\tilde{\pi}}$ indicates that actions are sampled $a_t\sim\tilde{\pi}(\cdot\|s_t)$. Let $\rho_\pi$ be the (unnormalized) discounted visitation frequencies  
  
$$\rho_\pi(s) = P(s_0=s) + \gamma P(s_1 =s) + \gamma^2P(s_2=s)+...,$$
  
where $s_0\sim\rho_0$ and the action are chosen according to $\pi$. We can rewrite Equation $(1)$ with the sum over states instead of timesteps:  
  
$$
\begin{aligned}
\eta(\tilde{\pi})&=\eta(\pi)+\sum_{t=0}^\infty\sum_sP(s_t=s|\tilde{\pi})\sum_a\tilde{\pi}(a|s)\gamma^tA_\pi(s,a)\\
&=\eta(\pi)+\sum_s\sum_{t=0}^\infty\gamma^tP(s_t=s|\tilde{\pi})\sum_a\tilde{\pi}(a|s)A_\pi(s,a)\\
&=\eta(\pi) + \sum_s\rho_{\tilde{\pi}}(s)\sum_a\tilde{\pi}(a|s)A_\pi(s,a)
\end{aligned}\tag{2}
$$

To solve the complex dependency of $\rho_{\tilde{\pi}}(s)$ on $\tilde{\pi}$ which makes Equation $(2)$ difficult to optimize directly. The author introduce the following local approximation to $\eta$:  

$$L_\pi(\tilde{\pi}) = \eta(\pi) + \sum_s\rho_\pi(s)\sum_a\tilde{\pi}(a|s)A_\pi(s,a)\tag{3}$$

Note that $L_\pi$ uses the visitation frequency $\rho_\pi$ rather than $\rho_{\tilde{\pi}}$, ignoring changes in state visitation density due to changes in the policy. However, if we have a parameterized policy $\pi_\theta$, where $\pi_\theta(a|s)$ is a differentiable function of the parameter vector $\theta$, then $L_\pi$ matches $\eta$ to first order. That is, for any parameter value $\theta_0$,  
  
$$
\begin{aligned}
L_{\pi_{\theta_0}}(\pi_{\theta_0}) &= \eta(\pi_{\theta_0})\\
\nabla_\theta L_{\pi_{\theta_0}}(\pi_\theta)|_{\theta = \theta_0}&=\nabla_\theta\eta(\pi_\theta)|_{\theta=\theta_0}
\end{aligned}\tag{4}
$$

Equation $(4)$ implies that a sufficiently small step $\pi_{\theta_0}\rightarrow\tilde{\pi}$ that improves $L_{\pi_{\theta_{\rm old}}}$ will also improve $\eta$.

---

## References
1. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2017). Trust Region Policy Optimization. [arXiv preprint arXiv:1502.05477v5.](https://arxiv.org/abs/1502.05477)  
2. Kakade S, Langford J. Approximately optimal approximate reinforcement learning[C]//Proceedings of the nineteenth international conference on machine learning. 2002: 267-274.