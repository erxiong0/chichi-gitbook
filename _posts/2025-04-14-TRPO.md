---
title: Trust Region Policy Optimization (TRPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Introduction

The **Trust Region Policy Optimization (TRPO)** algorithm[[1]](#references), introduced in 2017, addresses critical limitations of policy gradient methods in reinforcement learning by incorporating a **trust region constraint** to enable stable learning to enable stable learning of complex policies. During this period, policy optimization approaches predominantly fell into three categories:  
- *policy iteration methods* leveraging dynamic programming  
- *policy gradient methods* utilizing explicit gradient estimates  
- *derivative-free optimization methods*  
  
While gradient-based techniques had demonstrated success in training function approximators for supervised learning, they exhibited inconsistent performance in policy optimization scenarios, frequently underperforming gradient-free approaches like random search. This discrepancy stemmed from challenges including **high variance in gradient estimates** and **non-monotonic policy improvement** during iterative updates. TRPO resolve these issues by mathematically formalizing policy updates as a constrained optimization problem, where the objective function maximization is bounded by a **Kullback-Leibler (KL) divergence threshold**. This mechanism enforces localized updates within a trust region, balancing aggressive policy improvement with stability guarantees to prevent catastrophic performance collapse.

---

### Objective Function
The unified optimization objective integrates three key elements:  
    
$$L_t^{\text{CLIP+VF+S}}(\theta) = \mathbb{E}_t \left[ L_t^{\text{CLIP}}(\theta) - c_1 L_t^{\text{VF}}(\theta) + c_2 S[\pi_\theta](s_t) \right]$$

#### 1. Clipped Surrogate Objective  
  
$$L_t^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t,\ \text{clip}\left( r_t(\theta),\ 1-\epsilon,\ 1+\epsilon \right) \hat{A}_t \right) \right]$$  
  
where $r_t(\theta) = \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}$ is ratio paremeter, $\hat{A}_t$ is advantage estimate and $\epsilon \in [0.1, 0.3]$ a clipping threshold which contrains policy update in a safe limit.
  

#### 2. Value Function Loss  
  
$$L_t^{\text{VF}}(\theta) = \frac{1}{2} \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{targ}} \right)^2 \right]$$  
  
*Implementation Note*: The $\frac{1}{2}$ coefficient serves as a gradient scaling factor.  
  

#### 3. Entropy Regularization  
  
$$S[\pi_\theta](s_t) = -\sum_{a'} \pi_\theta(a'|s_t) \log \pi_\theta(a'|s_t)$$  
  
**Tips**:  
  Coefficients $c_1$ and $c_2$ balance the policy update, value function accuracy, and entropy bonus. Usually value loss weight is set to $c_1 \in [0.5, 1.0]$ and entropy bonus weight is $c_2 \in [0.01, 0.05]$.

---

### Architecture & Execution Pipeline  
  
#### Network Architecture Components
The policy network $\pi_\theta(a|s_t)$ and value function estimator $V_\theta(s_t)$ share a unified neural architecture with sepecialized components such as feature encoder
$h_t=f_\theta^{encoder}(s_t)$ with separate heads, e.g. policy head with $\pi_\theta(a|s_t)={\rm softmax}(W_\pi h_t+b_\pi)$ and output probability distribution over action $\mathcal{A}$ while value head maybe $V_\theta(s_t)=W_vh_t+b_v$ with a scalar value estimate.
  
---

### Execution Workflow

#### Phase 1: Experience Collection Pipeline  
During the rollout phase, agents interact with the environment using the old policy $\pi_{\text{old}}$ to generate trajectories $$\tau=\{(s_t, a_t, r_t)\}_{t=0}^{T-1}$$. Subsequently, the Generalized Advantage Estimation (GAE) method computes advantage values $\hat{A}_t$ through recursive temporal difference calculations:

$$
\begin{aligned}
\delta_t &= r_t + \gamma V_\theta(s_{t+1}) - V_\theta(s_t), \\
\hat{A}_t &= \sum_{\ell=0}^{T-t-1} (\gamma\lambda)^\ell \delta_{t+\ell},
\end{aligned}
$$

where $\gamma$ is the discount factor and $\lambda$ the GAE smoothing coefficient. Processed transitions $$(s_t, a_t, \hat{A}_t, V_t^{\mathrm{targ}})$$ — with target values $$V_t^{\mathrm{targ}}=\hat{A}_t+V_\theta(s_t)$$ — are stored in an experience replay buffer (capacity: $10^5 \sim 10^6$). This staged pipeline decouples data collection from policy updates while maintaining sample diversity through large-scale experience reuse.


#### Phase 2: Parameter Update
For $K$ optimization epochs:
1. *Mini-batch Sampling*: Uniformly sample $B$ transitions ($B \in [64, 2048]$) from buffer.

2. *Policy Optimization*: Compute gradients for the clipped surrogate objective:
$$\nabla_\theta L^{CLIP}=\mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, {\text clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\nabla_\theta\log\pi_\theta(a_t|s_t)\right]$$

3. *Value Network Update*: Minimize the value function's MSE loss through SGD:
$$\nabla_\theta L^{VF}=\mathbb{E}_{(s_t, V_t^{\text targ})\sim {\text buffer}}\left[(V_\theta(s_t)-V_t^{\text targ}\nabla_\theta V_\theta(s_t))\right]$$

4. *Entropy Adaptation* (Optional): Dynamically adjust $c_2$ based on entropy monitoring.

---

## References
1. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2017). Trust Region Policy Optimization. [arXiv preprint arXiv:1502.05477v5.](https://arxiv.org/abs/1502.05477)