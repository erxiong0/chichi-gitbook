---
title: Trust Region Policy Optimization (TRPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Introduction

The **Trust Region Policy Optimization (TRPO)** algorithm[[1]](#references), introduced in 2017, addresses critical limitations of policy gradient methods in reinforcement learning by incorporating a **trust region constraint** to enable stable learning of complex policies. 

---
### Main Idea  
In TRPO, an objective function is maximized subject to a constraint on the size of the policy update. Specifically,  
  
$$
\begin{aligned}
&\underset{\theta}{\rm maximize}\ \hat{\mathbb{E}}_t\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\rm old}}(a_t|s_t)}\hat{A_t}\right]\\
&{\rm subject\ to}\ \hat{\mathbb{E}}_t\left[{\rm KL}[\pi_{\theta_{\rm old}}(\cdot|s_t),\pi_\theta(\cdot|s_t)]\right]\leq\delta
\end{aligned}
$$

Here, $\theta_{\rm old}$ is the vector of policy parameters before the update. This problem can efficiently be approximately solved using the conjugate gradient algorithm, after making a linear approximation to the objective and a quadratic approximation to the constraint.  
  
From theory perspective, TRPO actually suggests using a penalty instead of a constraint, i.e., solving the unconstrained optimization problem  
  
$$\underset{\theta}{\rm maximize}\ \hat{\mathbb{E}}\left[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\rm old}}(a_t|s_t)}\hat{A}_t-\beta\ {\rm KL}\left[\pi_{\theta_{\rm old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)\right]\right]$$

for some coefficient $\beta$. TRPO uses a hard constraint rather than a penalty because it is hard to choose a single value of $\beta$ that performs well across different problems -- or even within a single problem, where the characteristics change over the course of learning. Hence, to achieve goal of a first-order algorithm that emulates the monotonic improvement of TRPO, experiments show that it is not sufficient to simply choose a fixed penalty coefficient $\beta$ and optimize the penalized objective equation with SGD; additional modifications are required.  
  
## Experiment

---

## References
1. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2017). Trust Region Policy Optimization. [arXiv preprint arXiv:1502.05477v5.](https://arxiv.org/abs/1502.05477)  