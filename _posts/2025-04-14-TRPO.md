---
title: Trust Region Policy Optimization (TRPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Introduction

The **Trust Region Policy Optimization (TRPO)** algorithm[[1]](#references), introduced in 2017, addresses critical limitations of policy gradient methods in reinforcement learning by incorporating a **trust region constraint** to enable stable learning to enable stable learning of complex policies. During this period, policy optimization approaches predominantly fell into three categories:  
- *policy iteration methods* leverages dynamic programming  
- *policy gradient methods* utilizes explicit gradient estimates  
- *derivative-free optimization methods*  
  
While gradient-based techniques had demonstrated success in training function approximators for supervised learning, they exhibited inconsistent performance in policy optimization scenarios, frequently underperforming gradient-free approaches like random search. This discrepancy stemmed from challenges including **high variance in gradient estimates** and **non-monotonic policy improvement** during iterative updates. TRPO resolve these issues by mathematically formalizing policy updates as a constrained optimization problem, where the objective function maximization is bounded by a **Kullback-Leibler (KL) divergence threshold**. This mechanism enforces localized updates within a trust region, balancing aggressive policy improvement with stability guarantees to prevent catastrophic performance collapse.

---

## References
1. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2017). Trust Region Policy Optimization. [arXiv preprint arXiv:1502.05477v5.](https://arxiv.org/abs/1502.05477)