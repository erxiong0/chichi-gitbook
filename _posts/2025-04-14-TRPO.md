---
title: Trust Region Policy Optimization (TRPO)
author: Chichi Syun
date: 2025-04-13
category: Reinforcement Learning
layout: post
---

## Introduction

The **Trust Region Policy Optimization (TRPO)** algorithm[[1]](#references), introduced in 2017, addresses critical limitations of policy gradient methods in reinforcement learning by incorporating a **trust region constraint** to enable stable learning to enable stable learning of complex policies. During this period, policy optimization approaches predominantly fell into three categories:  
- *policy iteration methods* leverages dynamic programming  
- *policy gradient methods* utilizes explicit gradient estimates  
- *derivative-free optimization methods*  
  
While gradient-based techniques had demonstrated success in training function approximators for supervised learning, they exhibited inconsistent performance in policy optimization scenarios, frequently underperforming gradient-free approaches like random search. This discrepancy stemmed from challenges including **high variance in gradient estimates** and **non-monotonic policy improvement** during iterative updates. TRPO resolve these issues by mathematically formalizing policy updates as a constrained optimization problem, where the objective function maximization is bounded by a **Kullback-Leibler (KL) divergence threshold**. This mechanism enforces localized updates within a trust region, balancing aggressive policy improvement with stability guarantees to prevent catastrophic performance collapse.

---
### Background  
Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple ($\mathcal{S}, \mathcal{A}, P, r, \rho_0, \gamma$), where  
- $\mathcal{S}$ is a finite set of states  
- $\mathcal{A}$ is a finite set of actions  
- $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$ is the transition probability distribution  
- $r: \mathcal{S}\rightarrow\mathbb{R}$ is the reward function  
- $\rho_0:\mathcal{S}\rightarrow\mathbb{R}$ is the distribution of the initial state $s_0$  
- $\gamma\in(0,1)$ is the discount factor.  
  
Let $\pi$ denote a stochastic policy $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0,1]$, and let $\eta(\pi)$ denote its expected discounted reward:  
  
$$\eta(\pi)=\mathbb{E}_{s_0,a_0,...}\left[\sum_{t=0}^\infty\gamma^tr(s_t)\right]$$

where    
$s_0\sim \rho_0(s_0),\ a_t\sim \pi(a_t|s_t),\ s_{t+1}\sim P(s_{t+1}|s_t, a_t)$.  
  
The state-action value function $Q_\pi$, the value function $V_\pi$, and the advantage function $A_\pi$ are defined as followed:  
    
$$
\begin{aligned}
Q_\pi(s_t, a_t) &= \mathbb{E}_{s_{t+1}, a_{t+1},...}\left[\sum_{k=0}^\infty\gamma^kr(s_{t+k})\right]\\
V_\pi(s_t)&=\mathbb{E}_{a_t, s_{t+1},...}\left[\sum_{k=0}^\infty\gamma^kr(s_{t+k})\right]\\
A_\pi(s,a)&=Q_\pi(s,a)-V_\pi(s)
\end{aligned}
$$

where,  
$a_t\sim \pi(a_t|s_t),\ s_{t+1}\sim P(s_{t+1}|s_t, a_t)$ for $t\geq 0$.  
  
There's useful identity expresses the expected return of another policy $\tilde{\pi}$ in terms of the advantage over $\pi$, accumulated over timesteps (see Kakade & Langford (2022)[[2]](#references))  
   
$$\eta(\tilde{\pi}) = \eta(\pi) + \mathbb{E}_{s_0,a_0,...\sim\tilde{\pi}}\left[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)\right]\tag{1}$$

  
where the notation $\mathbb{E}_{s_0, a_0, ...,\sim\tilde{\pi}}$ indicates that actions are sampled $$a_t\sim\tilde{\pi}(\cdot|s_t)$$.  
  
Let $\rho_\pi$ be the (unnormalized) discounted visitation frequencies  
  
$$\rho_\pi(s) = P(s_0=s) + \gamma P(s_1 =s) + \gamma^2P(s_2=s)+...,$$

where $s_0\sim\rho_0$ and the action are chosen according to $\pi$. We can rewrite Equation $(1)$ with the sum over states instead of timesteps:  
  
$$
\begin{aligned}
\eta(\tilde{\pi})&=\eta(\pi)+\sum_{t=0}^\infty\sum_sP(s_t=s|\tilde{\pi})\sum_a\tilde{\pi}(a|s)\gamma^tA_\pi(s,a)\\
&=\eta(\pi)+\sum_s\sum_{t=0}^\infty\gamma^tP(s_t=s|\tilde{\pi})\sum_a\tilde{\pi}(a|s)A_\pi(s,a)\\
&=\eta(\pi) + \sum_s\rho_{\tilde{\pi}}(s)\sum_a\tilde{\pi}(a|s)A_\pi(s,a)
\end{aligned}
$$

  
This equation implies that any policy update $$\pi\rightarrow \tilde{\pi}$$ that has a nonnegative expected advantage at every state $s$, i.e., $$\sum_a\tilde{\pi}(a|s)A_\pi(s,a)\geq 0$$, is guaranteed to increase the policy performance $\eta$, or leave it constant in the case that the expected advantage is zero everywhere.  

---

## References
1. Schulman, J., Levine, S., Moritz, P., Jordan, M. I., & Abbeel, P. (2017). Trust Region Policy Optimization. [arXiv preprint arXiv:1502.05477v5.](https://arxiv.org/abs/1502.05477)  
2. Kakade S, Langford J. Approximately optimal approximate reinforcement learning[C]//Proceedings of the nineteenth international conference on machine learning. 2002: 267-274.